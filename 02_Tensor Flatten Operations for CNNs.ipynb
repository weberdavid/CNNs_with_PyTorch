{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch only takes batches of pictures for training (in the format batchsize, colorchannel, height, width), which sometimes requires operations on them beforehand. As an example, those three Tensors represent three pictures 4x4. Those three Tensors must be combined to one, as a batch is one Tensor only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2248, 0.6739, 0.0654, 0.9750],\n",
      "        [0.0515, 0.1576, 0.7053, 0.6500],\n",
      "        [0.3019, 0.1109, 0.7773, 0.1881],\n",
      "        [0.6008, 0.5648, 0.3179, 0.4180]]) \n",
      " tensor([[0.8730, 0.2742, 0.3613, 0.2737],\n",
      "        [0.0917, 0.2331, 0.5329, 0.2045],\n",
      "        [0.9011, 0.1356, 0.9671, 0.5750],\n",
      "        [0.2757, 0.1584, 0.1826, 0.2419]]) \n",
      " tensor([[0.7059, 0.2426, 0.1750, 0.6246],\n",
      "        [0.7348, 0.7865, 0.0702, 0.7750],\n",
      "        [0.9697, 0.7059, 0.8368, 0.1521],\n",
      "        [0.9095, 0.3953, 0.5696, 0.8009]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(4, 4)\n",
    "t2 = torch.rand(4, 4)\n",
    "t3 = torch.rand(4, 4)\n",
    "print(t1, \"\\n\", t2, \"\\n\", t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2248, 0.6739, 0.0654, 0.9750],\n",
       "         [0.0515, 0.1576, 0.7053, 0.6500],\n",
       "         [0.3019, 0.1109, 0.7773, 0.1881],\n",
       "         [0.6008, 0.5648, 0.3179, 0.4180]],\n",
       "\n",
       "        [[0.8730, 0.2742, 0.3613, 0.2737],\n",
       "         [0.0917, 0.2331, 0.5329, 0.2045],\n",
       "         [0.9011, 0.1356, 0.9671, 0.5750],\n",
       "         [0.2757, 0.1584, 0.1826, 0.2419]],\n",
       "\n",
       "        [[0.7059, 0.2426, 0.1750, 0.6246],\n",
       "         [0.7348, 0.7865, 0.0702, 0.7750],\n",
       "         [0.9697, 0.7059, 0.8368, 0.1521],\n",
       "         [0.9095, 0.3953, 0.5696, 0.8009]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The stack function stacks all three tensors:\n",
    "t = torch.stack((t1, t2, t3))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Batchsize, height, width\n",
    "# colorchannel is missing, we need to insert that\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2248, 0.6739, 0.0654, 0.9750],\n",
       "          [0.0515, 0.1576, 0.7053, 0.6500],\n",
       "          [0.3019, 0.1109, 0.7773, 0.1881],\n",
       "          [0.6008, 0.5648, 0.3179, 0.4180]]],\n",
       "\n",
       "\n",
       "        [[[0.8730, 0.2742, 0.3613, 0.2737],\n",
       "          [0.0917, 0.2331, 0.5329, 0.2045],\n",
       "          [0.9011, 0.1356, 0.9671, 0.5750],\n",
       "          [0.2757, 0.1584, 0.1826, 0.2419]]],\n",
       "\n",
       "\n",
       "        [[[0.7059, 0.2426, 0.1750, 0.6246],\n",
       "          [0.7348, 0.7865, 0.0702, 0.7750],\n",
       "          [0.9697, 0.7059, 0.8368, 0.1521],\n",
       "          [0.9095, 0.3953, 0.5696, 0.8009]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inserting color channel\n",
    "t = t.reshape(3, 1, 4, 4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2248)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The first image in the batch:\n",
    "t[0]\n",
    "#The first colorchannel of the first image:\n",
    "t[0][0]\n",
    "#The first row of pixels in the first colorchannel of the first image:\n",
    "t[0][0][0]\n",
    "#The first pixel value of the first pixel row in the first colorchannel of the first image:\n",
    "t[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2248, 0.6739, 0.0654, 0.9750, 0.0515, 0.1576, 0.7053, 0.6500, 0.3019,\n",
      "         0.1109, 0.7773, 0.1881, 0.6008, 0.5648, 0.3179, 0.4180],\n",
      "        [0.8730, 0.2742, 0.3613, 0.2737, 0.0917, 0.2331, 0.5329, 0.2045, 0.9011,\n",
      "         0.1356, 0.9671, 0.5750, 0.2757, 0.1584, 0.1826, 0.2419],\n",
      "        [0.7059, 0.2426, 0.1750, 0.6246, 0.7348, 0.7865, 0.0702, 0.7750, 0.9697,\n",
      "         0.7059, 0.8368, 0.1521, 0.9095, 0.3953, 0.5696, 0.8009]]) \n",
      " torch.Size([3, 16])\n"
     ]
    }
   ],
   "source": [
    "#How to flatten the image tensor now?\n",
    "#Solution: Flattening each image while keeping the batch Tensor\n",
    "f = t.flatten(start_dim = 1) #start_dim tells with which Tensor to start with to flatten the axis (colorchannel),\n",
    "                                # batch axis is skipped so to say\n",
    "print(f, \"\\n\", f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2248, 0.6739, 0.0654, 0.9750, 0.0515, 0.1576, 0.7053, 0.6500, 0.3019,\n",
       "         0.1109, 0.7773, 0.1881, 0.6008, 0.5648, 0.3179, 0.4180],\n",
       "        [0.8730, 0.2742, 0.3613, 0.2737, 0.0917, 0.2331, 0.5329, 0.2045, 0.9011,\n",
       "         0.1356, 0.9671, 0.5750, 0.2757, 0.1584, 0.1826, 0.2419],\n",
       "        [0.7059, 0.2426, 0.1750, 0.6246, 0.7348, 0.7865, 0.0702, 0.7750, 0.9697,\n",
       "         0.7059, 0.8368, 0.1521, 0.9095, 0.3953, 0.5696, 0.8009]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same with reshape:\n",
    "g = t.reshape(t.shape[0], -1)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to do, when we have RGB images?\n",
    "For RGB images, the colorchannels line up in one axis, so get flattened as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipy_base",
   "language": "python",
   "name": "ipy_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
